{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhrh2mcmnpQLqqOF6FQLmh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3660c2e6d3a0409bb6d6fa44aa46d845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89c11da6c689400fa123618bc1ab6a23",
              "IPY_MODEL_567159770b754a6bb4bd8130d1c296b0",
              "IPY_MODEL_ca5eaa59257a4ced90aaa1a16e984416"
            ],
            "layout": "IPY_MODEL_596ab8bbed064c03817a590959f13195"
          }
        },
        "89c11da6c689400fa123618bc1ab6a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54446029b578434987e2d757a5d7e740",
            "placeholder": "​",
            "style": "IPY_MODEL_e0ffd00eec274545b5e346e1630ed497",
            "value": " 38%"
          }
        },
        "567159770b754a6bb4bd8130d1c296b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18186b5c03aa41ecbf7b4d2301bd50a1",
            "max": 178090079,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_138e437602984e74838cf145075a32f4",
            "value": 68067328
          }
        },
        "ca5eaa59257a4ced90aaa1a16e984416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c346ba0b7b40d5a41fd9cc6bd560d5",
            "placeholder": "​",
            "style": "IPY_MODEL_b9e2a3bc5a424dbf8542b650e81f94b8",
            "value": " 64.9M/170M [00:05&lt;00:06, 17.9MB/s]"
          }
        },
        "596ab8bbed064c03817a590959f13195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54446029b578434987e2d757a5d7e740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0ffd00eec274545b5e346e1630ed497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18186b5c03aa41ecbf7b4d2301bd50a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "138e437602984e74838cf145075a32f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3c346ba0b7b40d5a41fd9cc6bd560d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e2a3bc5a424dbf8542b650e81f94b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdrew213/docs/blob/master/helloworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Opertations\n",
        "\n",
        "Note: be sure to enable the GPU for your instance!\n",
        "\n",
        "1. Clone the git repo, but be sure to get the correct branch\n",
        "2. Copy utilities from the repo to the local directory\n",
        "3. Download preannotated dataset\n",
        "4. Download pytorch training code\n",
        "5. Train your maodel\n",
        "6. Pick a random sample from the training set and try it\n",
        "7. Grab an image from the internet and try that too!\n"
      ],
      "metadata": {
        "id": "B4QtLNnHUT4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OxepDhPMyoG-"
      },
      "outputs": [],
      "source": [
        "#!wget https://pytorch.org/tutorials/_static/tv-training-code.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "!unzip -qq PennFudanPed.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SWaP7ot1SLy",
        "outputId": "f098a23d-dd76-461c-8403-6cb1d27e5047"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-04 02:52:18--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53723336 (51M) [application/zip]\n",
            "Saving to: ‘PennFudanPed.zip’\n",
            "\n",
            "PennFudanPed.zip    100%[===================>]  51.23M  11.4MB/s    in 5.6s    \n",
            "\n",
            "2023-02-04 02:52:24 (9.21 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/vision.git --branch v0.8.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMP8iOCgyr-x",
        "outputId": "4d31776e-1758-4033-c76e-6d9abd619747"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 291337, done.\u001b[K\n",
            "remote: Counting objects: 100% (1129/1129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 291337 (delta 1033), reused 1112 (delta 1029), pack-reused 290208\u001b[K\n",
            "Receiving objects: 100% (291337/291337), 584.28 MiB | 16.85 MiB/s, done.\n",
            "Resolving deltas: 100% (266984/266984), done.\n",
            "Note: switching to '2f40a483d73018ae6e1488a484c5927f2b309969'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp vision/references/detection/utils.py .\n",
        "!cp vision/references/detection/transforms.py .\n",
        "!cp vision/references/detection/coco_eval.py .\n",
        "!cp vision/references/detection/engine.py .\n",
        "!cp vision/references/detection/coco_utils.py ."
      ],
      "metadata": {
        "id": "3sYxdN57zQ_M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample code from the TorchVision 0.3 Object Detection Finetuning Tutorial\n",
        "# http://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # train on the GPU or on the CPU, if a GPU is not available\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # our dataset has two classes only - background and person\n",
        "    num_classes = 2\n",
        "    # use our dataset and defined transformations\n",
        "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "    # split the dataset in train and test set\n",
        "    indices = torch.randperm(len(dataset)).tolist()\n",
        "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # get the model using our helper function\n",
        "    model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=3,\n",
        "                                                   gamma=0.1)\n",
        "\n",
        "    # let's train it for 10 epochs\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    print(\"That's it!\")\n",
        "    return model\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    mymodel = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "3660c2e6d3a0409bb6d6fa44aa46d845",
            "89c11da6c689400fa123618bc1ab6a23",
            "567159770b754a6bb4bd8130d1c296b0",
            "ca5eaa59257a4ced90aaa1a16e984416",
            "596ab8bbed064c03817a590959f13195",
            "54446029b578434987e2d757a5d7e740",
            "e0ffd00eec274545b5e346e1630ed497",
            "18186b5c03aa41ecbf7b4d2301bd50a1",
            "138e437602984e74838cf145075a32f4",
            "d3c346ba0b7b40d5a41fd9cc6bd560d5",
            "b9e2a3bc5a424dbf8542b650e81f94b8"
          ]
        },
        "id": "xRRDN3XVUQNh",
        "outputId": "122ba414-97e3-40b4-b57a-12b73b92509d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/170M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3660c2e6d3a0409bb6d6fa44aa46d845"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# pick one image from the test set\n",
        "img, _ = dataset_test[17]\n",
        "# put the model in evaluation mode\n",
        "mymodel.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = mymodel([img.to(device)])\n",
        "\n"
      ],
      "metadata": {
        "id": "7_qrlH6XWSB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "metadata": {
        "id": "At-sYhNDhzP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "metadata": {
        "id": "XcrL9tfwh1eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ak.picdn.net/offset/photos/5b89688fdaee26d9e8c7937d/medium/photo.jpg"
      ],
      "metadata": {
        "id": "vsT6qZFFj0Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('photo.jpg')\n",
        "img"
      ],
      "metadata": {
        "id": "wlL175gRkJTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define a transform to convert PIL \n",
        "# image to a Torch tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "  \n",
        "# transform = transforms.PILToTensor()\n",
        "# Convert the PIL image to Torch tensor\n",
        "img_tensor = transform(img)\n",
        "  \n",
        "# print the converted Torch tensor\n",
        "#print(img_tensor)\n",
        "img_tensor_float=img_tensor/255\n",
        "#print(img_tensor_float)\n",
        "\n",
        "mymodel.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = mymodel([img_tensor_float.to(device)])"
      ],
      "metadata": {
        "id": "Y16a1uAkkRNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "metadata": {
        "id": "zTv6XHZqlcmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTRqQyClsl9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}